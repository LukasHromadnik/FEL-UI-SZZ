# Neural Networks

Composition of simple linear or non-linear functions (neurons) parametrized by weights and biases.

$$\hat{y} = f\left( \langle \bm{w}, \bm{x} \rangle + b \right) = f \left( \langle \bm{w}, \bm{x} \rangle + w_0 \cdot 1 \right) = f \left( \langle \bm{w}', \bm{x}' \rangle \right)$$

**Logistic sigmoid.**
$$\sigma(s) = \frac{1}{1 + e^-s} = \frac{e^s}{e^s + 1}$$

**Rectified Linear Unit (ReLU).** Helps with vanishing gradients problem: the gradient is constant for $s > 0$, while for sigmoid-like activations it becomes increasingly small.
$$f(s) = \max(0, s)$$

## Linear Neuron

Single neuron with linear activation function $\equiv$ **linear regression**
$$\hat{y} = s = \langle \bm{x}, \bm{w} \rangle, \quad \hat{y} \in \mathbb{R}$$
For the whole dataset we get
$$\bm{\hat{y}} = \mathbf{X} \bm{w}, \quad \bm{\hat{y}} \in \mathbb{R}^m$$

### MLE

Likelihood for i.i.d. data
$$\pc[p]{\bm{y}}{\bm{w}, \mathbf{X}, \sigma} = \prod_{i=1}^m \pc[p]{y_i}{\bm{w}, \bm{x}_i, \sigma} = \left( 2 \pi \sigma^2 \right)^{-\frac{m}{2}} e^{-\frac{1}{2 \sigma^2} (\bm{y} - \mathbf{X} \bm{w})^T (\bm{y} - \mathbf{X} \bm{w})}$$
Negative Log Likelihood (switching to minimization)
$$\mathcal{L}(\bm{w}) = \frac{m}{2} \log \left( 2 \pi \sigma^2 \right) + \frac{1}{2 \sigma^2} (\bm{y} - \mathbf{X} \bm{w})^T (\bm{y} - \mathbf{X} \bm{w})$$
Note that
$$\sum_{i = 1}^m \underbrace{\left(y_i - \langle \bm{w}, \bm{x}_i \rangle \right)^2}_{\ell(y_i, \hat{y}_i)} = (\bm{y} - \mathbf{X} \bm{w})^T (\bm{y} - \mathbf{X} \bm{w})$$
is the **sum-of-squares** or **squared error** (SE).

Solving $\frac{\partial \mathcal{L}}{\partial \bm{w}} = 0$ we get $\bm{w}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \bm{y}$.

## Logistic Sigmoid

$$\hat{y} = \sigma(s), \quad \hat{y} \in (0, 1)$$

Sigmoid output can represent a parameter of the Bernoulli distribution
$$\pc[p]{y}{\hat{y}} = \pc[\mathrm{Ber}]{y}{\hat{y}} = \hat{y}^y (1 - \hat{y})^{1 - y} = \begin{cases}
\hat{y} & \text{for } y = 1 \\
1 - \hat{y} & \text{for } y = 0
\end{cases}
$$
Binary classifier
$$h(\hat{y}) = \begin{cases}
1 & \text{if } \hat{y} > \frac{1}{2} \\
0 & \text{else}
\end{cases}
$$

### MLE

$$\frac{\partial \mathcal{L}}{\partial s_i} = \hat{y}_i - y_i$$
Gradient w.r.t. logistic regression parameters
$$\frac{\partial \mathcal{L}}{\partial \bm{w}} = \sum_{i = 1}^m \frac{\partial \mathcal{L}}{\partial s_i} \cdot \frac{\partial s_i}{\partial \bm{w}} = \sum_{i = 1}^m \bm{x}_i (\hat{y}_i - y_i) = \mathbf{X}^T(\bm{\hat{y}} - \bm{y})$$

## Logistic Regression

Neuron using sigmoid activation function $\equiv$ **logistic regression**
$$\hat{y} = \sigma( \langle \bm{w}, \bm{x} \rangle), \quad \hat{y} \in (0, 1)$$

### MLE

$$\pc[p]{\bm{y}}{\bm{w}, \mathbf{X}} = \prod_{i = 1}^m \f{Ber}{y_i \given \hat{y}_i} = \prod_{i = 1}^m \hat{y}_i^{y_i} (1 - \hat{y}_i)^{1 - y_i}$$
Negative Log Likelihood
$$\mathcal{L}(\bm{w}) = \sum_{i = 1}^m \underbrace{- \left( y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i) \right)}_{\ell(y_i, \hat{y}_i)}$$
This *loss function* is called the **cross-entropy**.

## Softmax

$$\sigma_k(\bm{s}) = \frac{e^{s_k}}{\sum_{c = 1}^K e^{s_c}}$$

### MLE

$$\pc[p]{\bm{y}}{\bm{w}, \mathbf{X}} = \prod_{i = 1}^m \prod_{c = 1}^K \hat{y}_{ic}^{y_{ic}}$$
Negative Log Likelihood
$$\mathcal{L}(\bm{w}) = - \sum_{i = 1}^n \sum_{c = 1}^K y_{ic} \log (\hat{y}_{ic})$$

**Multinomial Logistic Regression**

* linear layer + softmax

$$\hat{y}_k = \sigma_k(\bm{x}^T \mathbf{W}), \quad h(\bm{x}, \mathbf{W}) = \argmax_k \hat{y}_k$$

## Loss functions

-------------------------------------------------------------------------------------------------------------------------------------
Problem                              Suggested loss function
------------------------------------ ------------------------------------------------------------------------------------------------
Binary classification                Cross entropy
                                     $$-\frac{1}{m} \sum_{i = 1}^m \left(y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i)\right)$$

Multinomial classification           Multinomial cross-entropy
                                     $$-\frac{1}{m} \sum_{i = 1}^m \sum_{c = 1}^K y_{ic} \log (\hat{y}_{ic})$$

Regression                           Mean squared error
                                     $$\frac{1}{m} \sum_{i = 1}^m (y_i - \hat{y}_i)^2$$

Multi-output regression              Mean squared error
                                     $$\frac{1}{m} \sum_{i = 1}^m \sum_{c = 1}^K (y_{ic} - \hat{y}_{ic})^2$$

-------------------------------------------------------------------------------------------------------------------------------------

## Regularization

Limit hypothesis space by limiting the size of the weight vector to prevent overfitting.

L2 regularization (weight decay): zero mean Gaussian prior, $\mathcal{N} \sim (0, \sigma_0^2)$.

## MAP Estimate

$$\bm{w}_{MAP} = \argmax_{\bm{w}} p(\bm{w} \given \bm{y}, \mathbf{X}) = \argmin_{\bm{w}} (- \log p(\bm{w} \given \bm{y}, \mathbf{X}))$$
where
$$-\log p(\bm{w} \given \bm{y}, \mathbf{X}) = \frac{1}{2 \sigma^2} (\bm{y} - \mathbf{X} \bm{w})^T (\bm{y} - \mathbf{X} \bm{w}) + \frac{1}{2 \sigma_0^2} \bm{w}^T \bm{w} + C$$
we can omit $C$, define $\lambda = \frac{\sigma^2}{\sigma_0^2}$ and minimize the loss function
$$\mathcal{L}(\bm{w}) = (\bm{y} - \mathbf{X} \bm{w})^T (\bm{y} - \mathbf{X} \bm{w}) + \lambda \bm{w}^T \bm{w}$$
The term $\lambda \bm{w}^T \bm{w} = \lambda \lVert \bm{w} \rVert_2^2$ minimizes the size of the weight vector.

## Other regularization related approaches

* L1 regularization: sum absolution values, i.e. use $\lambda \lVert \bm{w} \rVert_1$
* Randomize inputs: same as the weight decay for linear neurons
* Dataset augmentation
* Early stopping: start with small weights, stop when validation loss starts to grow, often used for limited time-budget
* Weight sharing and sparse connectivitiy: CNN
* Model averaging
* Dropout and DropConnect

## Backpropagation

A method to compute a gradient of the loss function with respect to its parameters: $\gradient \mathcal{L}(\bm{w})$ which is used  by optimization methods like gradient descent.

Let $\bm{\delta}^l = \frac{\partial \mathcal{L}}{\partial \bm{z}^l}$ be the sensitivity of the loss to the module input for layer $l$, then
$$\delta_i^l = \frac{\partial \mathcal{L}}{\partial z_i^l} = \sum_j \frac{\partial \mathcal{L}}{\partial z_j^{l + 1}} \cdot \frac{\partial z_j^{l + 1}}{\partial z_i^l} = \sum_j = \delta_j^{l + 1} \frac{\partial z_j^{l + 1}}{\partial z_i^l}$$
We need to know how to compute derivatives of outputs w.r.t. input only.

Similarly if the module has parameters we want to know how the loss changes w.r.t. them
$$\frac{\partial \mathcal{L}}{\partial w_i^l} = \sum_j \frac{\partial \mathcal{L}}{\partial z_j^{l + 1}} \cdot \frac{\partial z_j^{l + 1}}{\partial w_i^l} = \sum_j \delta_j^{l + 1} \frac{\partial z_j{^l + 1}}{\partial w_i^l}$$

So for each module we need only to specify there three messages

* **forward**: $\bm{z}^{l + 1} = \bm{f}(\bm{z}^l)$
* **backward**: $\displaystyle\frac{\partial \bm{z}^{l + 1}}{\partial \bm{z}^l}$
* **parameter** (optional): $\displaystyle\frac{\partial\bm{z}^{l + 1}}{\partial \bm{w}^l}$

### Example

Message   Linear Layer                                                                                           Mean Squared Error
---       ---                                                                                                    ---
Forward   $$z_j^{l + 1} = \sum_{i = 0}^n w_{ij} z_i^l, \quad j = 1, \dots, K$$                                   $$z^{l + 1} = \frac{1}{m} \sum_{i = 1}^m (y_i - z_i^l)^2$$
Backward  $$\frac{\partial z_j^{l + 1}}{\partial z_i^l} = w_{ij}, \quad i = 0, \dots, n, \quad j = 1, \dots, K$$ $$ \frac{\partial z^{l + 1}}{\partial z_i^l} = -\frac{2}{m}(y_i- z_i^l, \quad i = 1, \dots, n$$
Parameter $$\frac{\partial z_j^{l + 1}}{\partial w_{ik}} = \llbracket j = k \rrbracket z_i^l$$

## Gradient Descent

Find parameters which minimize loss over the training dataset
$$\bm{\theta}^* = \argmin_{\bm{\theta}} \mathcal{L}(\bm{\theta})$$
where $\bm{\theta}$ is a set of all parameters defining the ANN, e.g. all weight matrices and biases.

**Gradient descent.**
$$\bm{\theta}_{k + 1} = \bm{\theta}_k - \alpha_k \gradient \mathcal{L}(\bm{\theta}_k)$$
where $\alpha_k > 0$ is the **learning rate** or **stepsize** at iteration $k$.

## Stochastic Gradient Descent

\begin{algorithm}[h]
\caption{SGD Algorithm}
\begin{algorithmic}[1]
\State Choose an initial iterate $\bm{\theta}_1$
\For{$k = 1, 2, \dots$}
    \State Generate a realization of the random variable $o_k$
    \State Compute a stochastic vector $g(\bm{\theta}_k, o_k)$
    \State Choose a stepsize $\alpha_k > 0$
    \State Set the new iterate as $\bm{\theta}_{k + 1} \gets \bm{\theta}_k - \alpha_k g(\bm{\theta}_k, o_k)$
\EndFor
\end{algorithmic}
\end{algorithm}

Possible options of a stochastic vector
$$g(\bm{\theta}_k, o_k) = \begin{cases}
\gradient f(\bm{\theta}_k, o_k) & \text{single sample, online learning} \\
\frac{1}{m_k} \sum_{i = 1}^{m_k} \gradient f(\bm{\theta}_k, o_{k, i}) & \text{batch / mini-batch} \\
H_k \frac{1}{n_k} \sum_{i = 1}^{n_k} \gradient f(\bm{\theta}_k, o_{k,i}) & \text{Newton / quasi-Netwon direction}
\end{cases}
$$
